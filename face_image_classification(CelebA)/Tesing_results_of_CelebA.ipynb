{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tesing results of CelebA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python libraties\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "import argparse\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# pytorch libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms  # Transformations we can perform on our dataset\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn  # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
    "import torch.optim as optim  # For all Optimization algorithms, SGD, Adam, etc.\n",
    "import torch.nn.functional as F  # All functions that don't have any parameters\n",
    "from torch.utils.data import (\n",
    "    Dataset,\n",
    "    DataLoader,\n",
    ")  # Gives easier dataset managment and creates mini batches\n",
    "import torchvision.datasets as datasets  # Has standard datasets we can import in a nice way\n",
    "import torchvision.transforms as transforms  # Transformations we can perform on our dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "##########################  Data Preprocessing    #########################\n",
    "###########################################################################\n",
    "\n",
    "\n",
    "class CelebADataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, target, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.target = target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n",
    "        image = io.imread(img_path)\n",
    "        # 1=Smiling; 2=Attractive\n",
    "        if self.target == 'smile':\n",
    "            y_label = torch.tensor(int(self.annotations.iloc[index, 1]))\n",
    "        elif self.target == 'attractive':\n",
    "            y_label = torch.tensor(int(self.annotations.iloc[index, 2]))\n",
    "\n",
    "        bias = torch.tensor(int(self.annotations.iloc[index, 3]))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return (image, y_label, bias)\n",
    "    \n",
    "def Get_test_loader(data_dir, sensitive_type, img_type, target):\n",
    "    test_root_dir = data_dir + sensitive_type + '/' + img_type + '/test'\n",
    "\n",
    "\n",
    "    test_csv = data_dir + sensitive_type + '/' + sensitive_type +'_test.csv'\n",
    "\n",
    "\n",
    "    test_set = CelebADataset(\n",
    "        csv_file=test_csv,\n",
    "        root_dir=test_root_dir,\n",
    "        target = target,\n",
    "        transform=transforms.ToTensor(),\n",
    "    )\n",
    "\n",
    "    test_num = len(test_set)\n",
    "\n",
    "    test_loader = DataLoader(dataset=test_set, batch_size=1, shuffle=False)\n",
    "    return test_loader, test_num, test_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_saved_model_path(fairloss, data_dir, img_type, sensitive_type, target):\n",
    "    saved_dir = data_dir + sensitive_type + '/model/' + target\n",
    "\n",
    "    if fairloss:\n",
    "        save_file_name = saved_dir + '/' + img_type + f'_best_model+fairloss.pt'\n",
    "    else:\n",
    "        save_file_name = saved_dir + '/' + img_type + f'_best_model.pt'\n",
    "\n",
    "    return save_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yaoru\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\yaoru\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 2)\n",
    "model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy on training to see how good our model is\n",
    "\n",
    "def check_accuracy(predict_output, loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    i = 0\n",
    "        \n",
    "        \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y, bias in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "\n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "                \n",
    "            predict = predictions.data.cpu().numpy()\n",
    "            predict_output[i] = predict\n",
    "            i+=1\n",
    "\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "        print(\n",
    "            f\"Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}%\"\n",
    "        )\n",
    "\n",
    "        #model.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ypos_zpos(a,b):\n",
    "    if a==1 and b==1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def ypos_zneg(a,b):\n",
    "    if a==1 and b==0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_DP(predict_output, test_csv, sensitive_type, target):\n",
    "    predict_output_df = pd.DataFrame(predict_output, columns = ['predict'])\n",
    "    test = pd.read_csv(test_csv)\n",
    "\n",
    "    sensitive = test[sensitive_type]\n",
    "    frames = [predict_output_df, sensitive, test[target]]\n",
    "    result = pd.concat(frames, axis=1)\n",
    "    result['y=1z=1'] = result.apply(lambda result: ypos_zpos(result['predict'],result[sensitive_type]), axis=1)\n",
    "    result['y=1z=-1'] = result.apply(lambda result: ypos_zneg(result['predict'],result[sensitive_type]), axis=1)\n",
    "\n",
    "    if result[sensitive_type].value_counts().loc[0] == result[sensitive_type].value_counts().loc[1]:\n",
    "        pos_sensitive_num = result[sensitive_type].value_counts().loc[0]\n",
    "\n",
    "    DP = abs(result['y=1z=-1'].sum()/pos_sensitive_num - result['y=1z=1'].sum()/pos_sensitive_num)\n",
    "    print('The value of DP is',DP)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yhat1Y1Zn1(a,b,c):\n",
    "        if a==1 and b==1 and c==0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "def yhat1Y1Z1(a,b,c):\n",
    "    if a==1 and b==1 and c==1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def Y1Zn1(a,b):\n",
    "    if a==1 and b==0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def Y1Z1(a,b):\n",
    "    if a==1 and b==1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "def yhat1Yn1Zn1(a,b,c):\n",
    "    if a==1 and b==0 and c==0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def yhat1Yn1Z1(a,b,c):\n",
    "    if a==1 and b==0 and c==1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def Yn1Zn1(a,b):\n",
    "    if a==0 and b==0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def Yn1Z1(a,b):\n",
    "    if a==0 and b==1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_DEO(result, target, sensitive_type):\n",
    "    result['yhat1Y1Zn1'] = result.apply(lambda result: yhat1Y1Zn1(result['predict'],result[target],result[sensitive_type]), axis=1)\n",
    "    result['yhat1Y1Z1'] = result.apply(lambda result: yhat1Y1Z1(result['predict'],result[target],result[sensitive_type]), axis=1)\n",
    "\n",
    "    result['Y1Zn1'] = result.apply(lambda result: Y1Zn1(result[target],result[sensitive_type]), axis=1)\n",
    "    result['Y1Z1'] = result.apply(lambda result: Y1Z1(result[target],result[sensitive_type]), axis=1)\n",
    "\n",
    "    result['yhat1Yn1Zn1'] = result.apply(lambda result: yhat1Yn1Zn1(result['predict'],result[target],result[sensitive_type]), axis=1)\n",
    "    result['yhat1Yn1Z1'] = result.apply(lambda result: yhat1Yn1Z1(result['predict'],result[target],result[sensitive_type]), axis=1)\n",
    "\n",
    "    result['Yn1Zn1'] = result.apply(lambda result: Yn1Zn1(result[target],result[sensitive_type]), axis=1)\n",
    "    result['Yn1Z1'] = result.apply(lambda result: Yn1Z1(result[target],result[sensitive_type]), axis=1)\n",
    "\n",
    "    DEO = (abs(result['yhat1Y1Zn1'].sum()/result['Y1Zn1'].sum() - result['yhat1Y1Z1'].sum()/result['Y1Z1'].sum()) + \n",
    "        abs(result['yhat1Yn1Zn1'].sum()/result['Yn1Zn1'].sum() - result['yhat1Yn1Z1'].sum()/result['Yn1Z1'].sum()))\n",
    "\n",
    "    print('The value of DEO is',DEO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(data_dir, sensitive_type, img_type, target, fairloss):\n",
    "    test_loader, test_num, test_csv = Get_test_loader(data_dir, sensitive_type, img_type, target)\n",
    "    save_file_name = Get_saved_model_path(fairloss, data_dir, img_type, sensitive_type, target)\n",
    "    print('Evaluating Model')\n",
    "    model.load_state_dict(torch.load(save_file_name))\n",
    "    model.eval()\n",
    "\n",
    "    print(\"Checking accuracy on Test Set\")\n",
    "    predict_output = np.zeros((test_num, 1), dtype=int)\n",
    "    check_accuracy(predict_output, test_loader, model)\n",
    "\n",
    "    result = Get_DP(predict_output, test_csv, sensitive_type, target)\n",
    "\n",
    "    Get_DEO(result, target, sensitive_type)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Result (sensitive_type = 'gender', target = 'smile')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original images, no fairloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Model\n",
      "Checking accuracy on Test Set\n",
      "Got 1353 / 1500 with accuracy 90.20%\n",
      "The value of DP is 0.14\n",
      "The value of DEO is 0.09636653549314053\n"
     ]
    }
   ],
   "source": [
    "data_dir = './dataset/'\n",
    "sensitive_type = 'gender'\n",
    "img_type = 'origin'\n",
    "target = 'smile'\n",
    "fairloss = False\n",
    "\n",
    "eval(data_dir, sensitive_type, img_type, target, fairloss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grayscale images, no fairloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Model\n",
      "Checking accuracy on Test Set\n",
      "Got 1362 / 1500 with accuracy 90.80%\n",
      "The value of DP is 0.17866666666666664\n",
      "The value of DEO is 0.1460944332861715\n"
     ]
    }
   ],
   "source": [
    "data_dir = './dataset/'\n",
    "sensitive_type = 'gender'\n",
    "img_type = 'grey'\n",
    "target = 'smile'\n",
    "fairloss = False\n",
    "\n",
    "eval(data_dir, sensitive_type, img_type, target, fairloss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sketch images, no fairloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Model\n",
      "Checking accuracy on Test Set\n",
      "Got 1365 / 1500 with accuracy 91.00%\n",
      "The value of DP is 0.11066666666666669\n",
      "The value of DEO is 0.04136896406078278\n"
     ]
    }
   ],
   "source": [
    "data_dir = './dataset/'\n",
    "sensitive_type = 'gender'\n",
    "img_type = 'sketch'\n",
    "target = 'smile'\n",
    "fairloss = False\n",
    "\n",
    "eval(data_dir, sensitive_type, img_type, target, fairloss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sketch images with fairloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Model\n",
      "Checking accuracy on Test Set\n",
      "Got 1363 / 1500 with accuracy 90.87%\n",
      "The value of DP is 0.10533333333333333\n",
      "The value of DEO is 0.06818235299683663\n"
     ]
    }
   ],
   "source": [
    "data_dir = './dataset/'\n",
    "sensitive_type = 'gender'\n",
    "img_type = 'sketch'\n",
    "target = 'smile'\n",
    "fairloss = True\n",
    "\n",
    "eval(data_dir, sensitive_type, img_type, target, fairloss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Result (sensitive_type = 'skin_color', target = 'smile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original images, no fairloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Model\n",
      "Checking accuracy on Test Set\n",
      "Got 1385 / 1500 with accuracy 92.33%\n",
      "The value of DP is 0.16133333333333333\n",
      "The value of DEO is 0.06281971882238513\n"
     ]
    }
   ],
   "source": [
    "data_dir = './dataset/'\n",
    "sensitive_type = 'skin_color'\n",
    "img_type = 'origin'\n",
    "target = 'smile'\n",
    "fairloss = False\n",
    "\n",
    "eval(data_dir, sensitive_type, img_type, target, fairloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grayscale images, no fairloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Model\n",
      "Checking accuracy on Test Set\n",
      "Got 1374 / 1500 with accuracy 91.60%\n",
      "The value of DP is 0.13066666666666665\n",
      "The value of DEO is 0.06507360010183683\n"
     ]
    }
   ],
   "source": [
    "data_dir = './dataset/'\n",
    "sensitive_type = 'skin_color'\n",
    "img_type = 'grey'\n",
    "target = 'smile'\n",
    "fairloss = False\n",
    "\n",
    "eval(data_dir, sensitive_type, img_type, target, fairloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sketch images, no fairloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Model\n",
      "Checking accuracy on Test Set\n",
      "Got 1373 / 1500 with accuracy 91.53%\n",
      "The value of DP is 0.132\n",
      "The value of DEO is 0.028182534184150766\n"
     ]
    }
   ],
   "source": [
    "data_dir = './dataset/'\n",
    "sensitive_type = 'skin_color'\n",
    "img_type = 'sketch'\n",
    "target = 'smile'\n",
    "fairloss = False\n",
    "\n",
    "eval(data_dir, sensitive_type, img_type, target, fairloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sketch images with fairloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Model\n",
      "Checking accuracy on Test Set\n",
      "Got 1362 / 1500 with accuracy 90.80%\n",
      "The value of DP is 0.10399999999999998\n",
      "The value of DEO is 0.04951045177096439\n"
     ]
    }
   ],
   "source": [
    "data_dir = './dataset/'\n",
    "sensitive_type = 'skin_color'\n",
    "img_type = 'sketch'\n",
    "target = 'smile'\n",
    "fairloss = True\n",
    "\n",
    "eval(data_dir, sensitive_type, img_type, target, fairloss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Result (sensitive_type = 'hair_color', target = 'smile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original images, no fairloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Model\n",
      "Checking accuracy on Test Set\n",
      "Got 1369 / 1500 with accuracy 91.27%\n",
      "The value of DP is 0.061333333333333295\n",
      "The value of DEO is 0.05481875442089432\n"
     ]
    }
   ],
   "source": [
    "data_dir = './dataset/'\n",
    "sensitive_type = 'hair_color'\n",
    "img_type = 'origin'\n",
    "target = 'smile'\n",
    "fairloss = False\n",
    "\n",
    "eval(data_dir, sensitive_type, img_type, target, fairloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grayscale images, no fairloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Model\n",
      "Checking accuracy on Test Set\n",
      "Got 1376 / 1500 with accuracy 91.73%\n",
      "The value of DP is 0.05199999999999999\n",
      "The value of DEO is 0.03475528871241852\n"
     ]
    }
   ],
   "source": [
    "data_dir = './dataset/'\n",
    "sensitive_type = 'hair_color'\n",
    "img_type = 'grey'\n",
    "target = 'smile'\n",
    "fairloss = False\n",
    "\n",
    "eval(data_dir, sensitive_type, img_type, target, fairloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sketch images, no fairloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Model\n",
      "Checking accuracy on Test Set\n",
      "Got 1356 / 1500 with accuracy 90.40%\n",
      "The value of DP is 0.05466666666666664\n",
      "The value of DEO is 0.04265812806045176\n"
     ]
    }
   ],
   "source": [
    "data_dir = './dataset/'\n",
    "sensitive_type = 'hair_color'\n",
    "img_type = 'sketch'\n",
    "target = 'smile'\n",
    "fairloss = False\n",
    "\n",
    "eval(data_dir, sensitive_type, img_type, target, fairloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sketch images with fairloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Model\n",
      "Checking accuracy on Test Set\n",
      "Got 1337 / 1500 with accuracy 89.13%\n",
      "The value of DP is 0.037333333333333385\n",
      "The value of DEO is 0.012225679468179748\n"
     ]
    }
   ],
   "source": [
    "data_dir = './dataset/'\n",
    "sensitive_type = 'hair_color'\n",
    "img_type = 'sketch'\n",
    "target = 'smile'\n",
    "fairloss = True\n",
    "\n",
    "eval(data_dir, sensitive_type, img_type, target, fairloss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Result (sensitive_type = 'gender', target = 'attractive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original images, no fairloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Model\n",
      "Checking accuracy on Test Set\n",
      "Got 1219 / 1500 with accuracy 81.27%\n",
      "The value of DP is 0.5066666666666666\n",
      "The value of DEO is 0.5863539565653754\n"
     ]
    }
   ],
   "source": [
    "data_dir = './dataset/'\n",
    "sensitive_type = 'gender'\n",
    "img_type = 'origin'\n",
    "target = 'attractive'\n",
    "fairloss = False\n",
    "\n",
    "eval(data_dir, sensitive_type, img_type, target, fairloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grayscale images, no fairloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Model\n",
      "Checking accuracy on Test Set\n",
      "Got 1225 / 1500 with accuracy 81.67%\n",
      "The value of DP is 0.4826666666666667\n",
      "The value of DEO is 0.52323491554943\n"
     ]
    }
   ],
   "source": [
    "data_dir = './dataset/'\n",
    "sensitive_type = 'gender'\n",
    "img_type = 'grey'\n",
    "target = 'attractive'\n",
    "fairloss = False\n",
    "\n",
    "eval(data_dir, sensitive_type, img_type, target, fairloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sketch images, no fairloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Model\n",
      "Checking accuracy on Test Set\n",
      "Got 1184 / 1500 with accuracy 78.93%\n",
      "The value of DP is 0.42533333333333334\n",
      "The value of DEO is 0.44566689546189825\n"
     ]
    }
   ],
   "source": [
    "data_dir = './dataset/'\n",
    "sensitive_type = 'gender'\n",
    "img_type = 'sketch'\n",
    "target = 'attractive'\n",
    "fairloss = False\n",
    "\n",
    "eval(data_dir, sensitive_type, img_type, target, fairloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sketch images with fairloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Model\n",
      "Checking accuracy on Test Set\n",
      "Got 941 / 1500 with accuracy 62.73%\n",
      "The value of DP is 0.16533333333333333\n",
      "The value of DEO is 0.18046849180306154\n"
     ]
    }
   ],
   "source": [
    "data_dir = './dataset/'\n",
    "sensitive_type = 'gender'\n",
    "img_type = 'sketch'\n",
    "target = 'attractive'\n",
    "fairloss = True\n",
    "\n",
    "eval(data_dir, sensitive_type, img_type, target, fairloss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Result (sensitive_type = 'skin_color', target = 'attractive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original images, no fairloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Model\n",
      "Checking accuracy on Test Set\n",
      "Got 1214 / 1500 with accuracy 80.93%\n",
      "The value of DP is 0.27866666666666673\n",
      "The value of DEO is 0.3371141317937657\n"
     ]
    }
   ],
   "source": [
    "data_dir = './dataset/'\n",
    "sensitive_type = 'skin_color'\n",
    "img_type = 'origin'\n",
    "target = 'attractive'\n",
    "fairloss = False\n",
    "\n",
    "eval(data_dir, sensitive_type, img_type, target, fairloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grayscale images, no fairloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Model\n",
      "Checking accuracy on Test Set\n",
      "Got 1204 / 1500 with accuracy 80.27%\n",
      "The value of DP is 0.31066666666666665\n",
      "The value of DEO is 0.35624586556814586\n"
     ]
    }
   ],
   "source": [
    "data_dir = './dataset/'\n",
    "sensitive_type = 'skin_color'\n",
    "img_type = 'grey'\n",
    "target = 'attractive'\n",
    "fairloss = False\n",
    "\n",
    "eval(data_dir, sensitive_type, img_type, target, fairloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sketch images, no fairloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Model\n",
      "Checking accuracy on Test Set\n",
      "Got 1171 / 1500 with accuracy 78.07%\n",
      "The value of DP is 0.19466666666666665\n",
      "The value of DEO is 0.20403768729511246\n"
     ]
    }
   ],
   "source": [
    "data_dir = './dataset/'\n",
    "sensitive_type = 'skin_color'\n",
    "img_type = 'sketch'\n",
    "target = 'attractive'\n",
    "fairloss = False\n",
    "\n",
    "eval(data_dir, sensitive_type, img_type, target, fairloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sketch images with fairloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Model\n",
      "Checking accuracy on Test Set\n",
      "Got 1112 / 1500 with accuracy 74.13%\n",
      "The value of DP is 0.15066666666666673\n",
      "The value of DEO is 0.17798496236062156\n"
     ]
    }
   ],
   "source": [
    "data_dir = './dataset/'\n",
    "sensitive_type = 'skin_color'\n",
    "img_type = 'sketch'\n",
    "target = 'attractive'\n",
    "fairloss = True\n",
    "\n",
    "eval(data_dir, sensitive_type, img_type, target, fairloss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Result (sensitive_type = 'hair_color', target = 'attractive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original images, no fairloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Model\n",
      "Checking accuracy on Test Set\n",
      "Got 1203 / 1500 with accuracy 80.20%\n",
      "The value of DP is 0.07066666666666666\n",
      "The value of DEO is 0.09160565202632534\n"
     ]
    }
   ],
   "source": [
    "data_dir = './dataset/'\n",
    "sensitive_type = 'hair_color'\n",
    "img_type = 'origin'\n",
    "target = 'attractive'\n",
    "fairloss = False\n",
    "\n",
    "eval(data_dir, sensitive_type, img_type, target, fairloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grayscale images, no fairloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Model\n",
      "Checking accuracy on Test Set\n",
      "Got 1187 / 1500 with accuracy 79.13%\n",
      "The value of DP is 0.046666666666666634\n",
      "The value of DEO is 0.14120055532412984\n"
     ]
    }
   ],
   "source": [
    "data_dir = './dataset/'\n",
    "sensitive_type = 'hair_color'\n",
    "img_type = 'grey'\n",
    "target = 'attractive'\n",
    "fairloss = False\n",
    "\n",
    "eval(data_dir, sensitive_type, img_type, target, fairloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sketch images, no fairloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Model\n",
      "Checking accuracy on Test Set\n",
      "Got 1169 / 1500 with accuracy 77.93%\n",
      "The value of DP is 0.022666666666666613\n",
      "The value of DEO is 0.08486769810143865\n"
     ]
    }
   ],
   "source": [
    "data_dir = './dataset/'\n",
    "sensitive_type = 'hair_color'\n",
    "img_type = 'sketch'\n",
    "target = 'attractive'\n",
    "fairloss = False\n",
    "\n",
    "eval(data_dir, sensitive_type, img_type, target, fairloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sketch images with fairloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Model\n",
      "Checking accuracy on Test Set\n",
      "Got 1165 / 1500 with accuracy 77.67%\n",
      "The value of DP is 0.03333333333333327\n",
      "The value of DEO is 0.05636236095238728\n"
     ]
    }
   ],
   "source": [
    "data_dir = './dataset/'\n",
    "sensitive_type = 'hair_color'\n",
    "img_type = 'sketch'\n",
    "target = 'attractive'\n",
    "fairloss = True\n",
    "\n",
    "eval(data_dir, sensitive_type, img_type, target, fairloss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
